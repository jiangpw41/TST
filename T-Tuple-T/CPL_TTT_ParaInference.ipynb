{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangpeiwen2/jiangpeiwen2/miniconda3/envs/vllm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-14 21:29:53,270\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 12-14 21:29:55 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.\n",
      "INFO 12-14 21:29:55 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory\n",
      "INFO 12-14 21:29:55 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.\n",
      "{'engine_config': {'vllm': {'best_of': 2, 'dtype': 'float16', 'gpu_memory_utilization': 0.9, 'max_model_len': 4096, 'max_tokens': 1024, 'seed': 32, 'stop': ['<|im_end|>', '<|endoftext|>', '<|im_start|>'], 'temperature': 0, 'top_p': 1, 'trust_remote_code': True, 'use_beam_search': True}}, 'local': {'Baichuan2-13B-Chat': '/home/jiangpeiwen2/jiangpeiwen2/workspace/LLMs/Baichuan2-13B/Baichuan2-13B-Chat', 'Baichuan2-7B-Chat': '/home/jiangpeiwen2/jiangpeiwen2/workspace/LLMs/Baichuan2-7B-Chat', 'CPL_dynamic_counter_ChatGLM3-6B-3epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/CPL_dynamic/v1/models/0', 'CPL_dynamic_counter_Chinese-Mistral-7B-Instruct-v0.1-3epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/CPL_dynamic/v1/models/1', 'CPL_dynamic_counter_Qwen1.5-7B-Chat-3epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/CPL_dynamic/v1/models/2', 'CPL_dynamic_tabel_ChatGLM3-6B-4epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/CPL_dynamic/v1/models/3', 'CPL_dynamic_tabel_Chinese-Mistral-7B-Instruct-v0.1-4epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/CPL_dynamic/v1/models/4', 'CPL_dynamic_tabel_Qwen1.5-7B-Chat-4epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/CPL_dynamic/v1/models/5', 'CPL_static4baseline_Chinese-Mistral-7B-Instruct-v0.1-1.5epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/CPL_static/v2_dynamic_baseline/models/2', 'CPL_static4baseline_Qwen1.5-7B-Chat-1epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/CPL_static/v2_dynamic_baseline/models/1', 'ChatGLM3-6B': '/home/jiangpeiwen2/jiangpeiwen2/workspace/LLMs/ChatGLM3-6B/ZhipuAI/chatglm3-6b', 'ChatGLM3-6B-livesum-v1-20epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/LiveSum/v1/models/0', 'ChatGLM3-6B-static4dynamicbaseline-3epoches': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/CPL_static/v2_dynamic_baseline/models/0', 'ChatGLM3-6B-v1-20epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/CPL/v1/models/1', 'ChatGLM3-6B-v1-3epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/CPL/v1/models/0', 'ChatGLM3-6B-v1-4epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/CPL/v1/models/2', 'Chinese-Mistral-7B-Instruct-v0.1': '/home/jiangpeiwen2/jiangpeiwen2/workspace/LLMs/Chinese-Mistral-7B-Instruct-v0.1', 'Llama-2-7b-chat': '/home/jiangpeiwen2/jiangpeiwen2/workspace/LLMs/Llama-2-7b-chat-hf', 'Llama-3-8B-Instruct': '/home/jiangpeiwen2/jiangpeiwen2/workspace/LLMs/Meta-Llama-3-8B-Instruct', 'Llama-3.2-3B-Instruct': '/home/jiangpeiwen2/jiangpeiwen2/workspace/LLMs/Llama-3.2-3B-Instruct', 'Llama3-Chinese-8B-Instruct': '/home/jiangpeiwen2/jiangpeiwen2/workspace/LLMs/Llama3-Chinese-8B-Instruct', 'Meta-Llama-3-8B-Instruct-livesum-v1-40epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/LiveSum/v1/models/1', 'Mistral-7B-Instruct-v0.2': '/home/jiangpeiwen2/jiangpeiwen2/workspace/LLMs/Mistral-7B-Instruct-v0.2', 'Mistral-7B-Instruct-v0.2-livesum-30epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/LiveSum/v1/models/2', 'Qwen1.5-7B-Chat': '/home/jiangpeiwen2/jiangpeiwen2/workspace/LLMs/Qwen1.5-7B-Chat', 'Qwen1.5-7B-Chat-livesum-30epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/LiveSum/v1/models/4', 'Qwen2.5-0.5B': '/home/jiangpeiwen2/jiangpeiwen2/workspace/LLMs/Qwen2.5-0.5B', 'Qwen2.5-0.5B-livesum-30epoch': '/home/jiangpeiwen2/jiangpeiwen2/TKGT/test/LiveSum/v1/models/3'}, 'remote': {'NL2GQL': {'api_key': 'sk-t4Mv9tJa0ftMCcKqKMAlqJmq3x5Da83Pk4U4Jq2M98C57GZG', 'base_url': 'https://api.pro365.top/v1', 'model_list': ['gpt-3.5-turbo', 'gpt-3.5-turbo-1106', 'gpt-4', 'gpt-4-0125-preview', 'gpt-4-turbo', 'gpt-4o', 'gpt-4o-2024-08-06', 'claude-2', 'claude-3-5-sonnet', 'llama-3-70b']}}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from wayne_utils import load_data, save_data\n",
    "_ROOT_PATH = \"/home/jiangpeiwen2/jiangpeiwen2/TKGT\"\n",
    "from llm_inferencer import Register, Inferencer\n",
    "reg = Register()\n",
    "reg.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本地推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.add_local_model( \"CPL_dynamic_tabel_TTT_ChatGLM3-6B-5epoch\", \"/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/0\")\n",
    "reg.add_local_model( \"CPL_dynamic_tabel_Chinese-Mistral-7B-Instruct-v0.1-5epoch\", \"/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPL_dynamic_tabel_TTT_ChatGLM3-6B-5epoch',\n",
       " 'CPL_dynamic_tabel_Chinese-Mistral-7B-Instruct-v0.1-5epoch']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list_table = [\n",
    "    \"ChatGLM3-6B\",\n",
    "    \"Qwen1.5-7B-Chat\",\n",
    "    \"Baichuan2-7B-Chat\",\n",
    "    \"Chinese-Mistral-7B-Instruct-v0.1\",\n",
    "    \"Qwen2.5-0.5B\",\n",
    "    \"CPL_dynamic_tabel_TTT_ChatGLM3-6B-5epoch\",\n",
    "    \"CPL_dynamic_tabel_Chinese-Mistral-7B-Instruct-v0.1-5epoch\"\n",
    "]\n",
    "scopes = model_list_table[5:]\n",
    "scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPL_dynamic_tabel_TTT_ChatGLM3-6B-5epoch开始！\n",
      "Enginevllm exists, run with vllm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-14 21:30:56 llm_engine.py:75] Initializing an LLM engine (v0.4.0) with config: model='/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/0', tokenizer='/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/0', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=32)\n",
      "INFO 12-14 21:30:56 llm_engine.py:75] Initializing an LLM engine (v0.4.0) with config: model='/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/0', tokenizer='/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/0', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=32)\n",
      "INFO 12-14 21:30:56 llm_engine.py:75] Initializing an LLM engine (v0.4.0) with config: model='/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/0', tokenizer='/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/0', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=32)\n",
      "WARNING 12-14 21:30:56 tokenizer.py:104] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "WARNING 12-14 21:30:56 tokenizer.py:104] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "WARNING 12-14 21:30:57 tokenizer.py:104] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 12-14 21:30:57 selector.py:16] Using FlashAttention backend.\n",
      "INFO 12-14 21:30:57 selector.py:16] Using FlashAttention backend.\n",
      "INFO 12-14 21:30:57 selector.py:16] Using FlashAttention backend.\n",
      "INFO 12-14 21:31:01 model_runner.py:104] Loading model weights took 11.6579 GB\n",
      "INFO 12-14 21:31:03 gpu_executor.py:94] # GPU blocks: 20058, # CPU blocks: 9362\n",
      "INFO 12-14 21:31:03 model_runner.py:104] Loading model weights took 11.6579 GB\n",
      "INFO 12-14 21:31:03 model_runner.py:104] Loading model weights took 11.6579 GB\n",
      "INFO 12-14 21:31:04 gpu_executor.py:94] # GPU blocks: 20058, # CPU blocks: 9362\n",
      "INFO 12-14 21:31:04 gpu_executor.py:94] # GPU blocks: 20058, # CPU blocks: 9362\n",
      "INFO 12-14 21:31:06 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-14 21:31:06 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-14 21:31:07 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-14 21:31:07 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-14 21:31:07 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-14 21:31:07 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-14 21:31:12 model_runner.py:867] Graph capturing finished in 6 secs.\n",
      "No.5 LLM Inderence start!\n",
      "第5部分：:   0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-14 21:31:13 model_runner.py:867] Graph capturing finished in 6 secs.\n",
      "No.3 LLM Inderence start!\n",
      "第3部分：:   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-14 21:31:13 model_runner.py:867] Graph capturing finished in 6 secs.\n",
      "No.4 LLM Inderence start!\n",
      "第4部分：:   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第5部分：:   4%|▍         | 3/71 [00:29<11:09,  9.85s/it]WARNING 12-14 21:31:41 scheduler.py:243] Input prompt (4890 tokens) is too long and exceeds limit of 4096\n",
      "第5部分：: 100%|██████████| 71/71 [11:24<00:00,  9.65s/it]\n",
      "第4部分：: 100%|██████████| 70/70 [12:17<00:00, 10.53s/it]\n",
      "第3部分：: 100%|██████████| 70/70 [12:38<00:00, 10.84s/it]\n",
      "Model inference finished！\n",
      "CPL_dynamic_tabel_Chinese-Mistral-7B-Instruct-v0.1-5epoch开始！\n",
      "Enginevllm exists, run with vllm\n",
      "WARNING 12-14 21:43:53 config.py:748] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 12-14 21:43:53 config.py:748] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 12-14 21:43:53 config.py:748] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 12-14 21:43:53 llm_engine.py:75] Initializing an LLM engine (v0.4.0) with config: model='/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/1', tokenizer='/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=32)\n",
      "INFO 12-14 21:43:53 llm_engine.py:75] Initializing an LLM engine (v0.4.0) with config: model='/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/1', tokenizer='/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=32)\n",
      "INFO 12-14 21:43:53 llm_engine.py:75] Initializing an LLM engine (v0.4.0) with config: model='/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/1', tokenizer='/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/models/1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=32)\n",
      "INFO 12-14 21:43:53 selector.py:16] Using FlashAttention backend.\n",
      "INFO 12-14 21:43:53 selector.py:16] Using FlashAttention backend.\n",
      "INFO 12-14 21:43:53 selector.py:16] Using FlashAttention backend.\n",
      "INFO 12-14 21:44:01 model_runner.py:104] Loading model weights took 13.9849 GB\n",
      "INFO 12-14 21:44:02 model_runner.py:104] Loading model weights took 13.9849 GB\n",
      "INFO 12-14 21:44:02 model_runner.py:104] Loading model weights took 13.9849 GB\n",
      "INFO 12-14 21:44:03 gpu_executor.py:94] # GPU blocks: 3244, # CPU blocks: 2048\n",
      "INFO 12-14 21:44:03 gpu_executor.py:94] # GPU blocks: 3244, # CPU blocks: 2048\n",
      "INFO 12-14 21:44:03 gpu_executor.py:94] # GPU blocks: 3244, # CPU blocks: 2048\n",
      "INFO 12-14 21:44:04 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-14 21:44:04 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-14 21:44:05 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-14 21:44:05 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-14 21:44:05 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-14 21:44:05 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-14 21:44:11 model_runner.py:867] Graph capturing finished in 6 secs.\n",
      "No.4 LLM Inderence start!\n",
      "第4部分：:   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-14 21:44:11 model_runner.py:867] Graph capturing finished in 6 secs.\n",
      "No.5 LLM Inderence start!INFO 12-14 21:44:11 model_runner.py:867] Graph capturing finished in 6 secs.\n",
      "\n",
      "第5部分：:   0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.3 LLM Inderence start!\n",
      "第3部分：:   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第5部分：:   4%|▍         | 3/71 [00:19<09:18,  8.21s/it]WARNING 12-14 21:44:31 scheduler.py:243] Input prompt (4879 tokens) is too long and exceeds limit of 4096\n",
      "第5部分：: 100%|██████████| 71/71 [05:53<00:00,  4.97s/it]\n",
      "第4部分：: 100%|██████████| 70/70 [06:03<00:00,  5.19s/it]\n",
      "第3部分：: 100%|██████████| 70/70 [06:08<00:00,  5.26s/it]\n",
      "Model inference finished！\n"
     ]
    }
   ],
   "source": [
    "for model_name in scopes:\n",
    "    print(f\"{model_name}开始！\")\n",
    "    kwargs = {}\n",
    "    prompt_list_from_path = \"/home/jiangpeiwen2/jiangpeiwen2/T-Tuple-T/CPL_prompt_list.pickle\"\n",
    "    kwargs[\"local_or_remote\"] = \"local\"\n",
    "    kwargs[\"server_or_reader\"] = \"reader\"\n",
    "    kwargs[\"model_name\"] = model_name\n",
    "    kwargs[\"gpu_list\"] = \"3,4,5\"\n",
    "    kwargs[\"local_engine\"] = \"vllm\"\n",
    "    if prompt_list_from_path != None:\n",
    "        kwargs[\"prompt_list_from_path\"] = prompt_list_from_path\n",
    "        kwargs[\"predict_list_to_path\"] = prompt_list_from_path.replace( \"CPL_prompt_list.pickle\", f\"CPL_predict_list{model_name}.pickle\")\n",
    "        kwargs[\"sample_little\"] = None\n",
    "    inferencer = Inferencer( kwargs )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
